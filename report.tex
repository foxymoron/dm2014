\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Data Mining: Learning from Large Data Sets - Spring Semester 2014}
\author{aludovic@student.ethz.ch\\ jdixit@student.ethz.ch\\ rsridhar@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section{Approximate near-duplicate search using Locality Sensitive Hashing} 

We used the numpy python library for this project. For the mapper we consider each space delimited number as a shingle. Since we had to generate a maximum of 256 hash functions, we use the function randint() to generate random integers for our hash functions. We save them all in a vector and we define a signature vector to the biggest integer value possible.

We then implement the minhash algorithm using bandwidth=15 with each 17 rows (except the last one with 16 rows) and then we eventually calculate a key for each video. We send to the reducer the key and respective video details.

In the reducer part we calculate the Jaccard similarity between videos which have the same keys and see if its more than 85 percent similar, and output it to the file separated by a tab.



\section{Large-Scale Image Classification}

We used numpy and scikit learn python libraries for this project. In the mapper we used SGD classifier to run a partial fit on a batch of 50 rows of data at a time. No transformation is done. The calculated weights including the intercept are sent to the reducer that adds them up and normalizes them in order to obtain the actual weights for this model.

\section{Extracting Representative Elements From Large Datasets}

For this project we used the numpy python library. In the mapper we get a set of 8000 at a time to form coresets. Once we get those

we assign them random initial clusters. On those points we create coresets and initialize their weights to zero. For every point

in a batch we initilize the minimum index to 4242 and minimum distance to infinity and we try to calculate the weight of each coreset

and send them both to the reducer. In the reducer we use these coresets theselves as points and recluster them using online k means and coreset weights.The number of clusters

used here is 200.

\section{Explore-Exploit Tradeoffs in Recommender Systems}


\end{document} 
